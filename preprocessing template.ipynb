{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preprocessing Tools**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TeVgITB32Hxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Importation de donnees**"
      ],
      "metadata": {
        "id": "B8bfZWba1ltz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#importer le dataset\n",
        "dataset_path='Life Expectancy Data.csv'\n",
        "df=pd.read_csv(dataset_path)\n",
        "print('le dataset a ete importe avec success')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DBk0Lrq2i3j",
        "outputId": "cbcbc03c-9c9b-471b-ad5a-341140693647"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "le dataset a ete importe avec success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.0 Nettoyage des donnees**"
      ],
      "metadata": {
        "id": "bAM2082J3e7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#identifier et supprimer les colonnes qui contient une seule valeur\n",
        "\n",
        "\n",
        "#afficher les nombre de valeurs unique pour chaque colonne\n",
        "print(df.nunique())\n",
        "\n",
        "#identifier les colonne avec 1 valeur unique et les supprimer\n",
        "columns_to_drop = df.columns[df.nunique() == 1]\n",
        "print(\"\\n les colonnes avec 1 seul valeur unique sont : \",columns_to_drop)\n",
        "\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "print(\"\\n les colonnes avec 1 seul valeur unique ont ete supprimee\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56B0Bm8D36X6",
        "outputId": "ab576346-25b8-4e75-99df-0690aa9eb081"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Country                             193\n",
            "Year                                 16\n",
            "Status                                2\n",
            "Life expectancy                     362\n",
            "Adult Mortality                     425\n",
            "infant deaths                       209\n",
            "Alcohol                            1076\n",
            "percentage expenditure             2328\n",
            "Hepatitis B                          87\n",
            "Measles                             958\n",
            " BMI                                608\n",
            "under-five deaths                   252\n",
            "Polio                                73\n",
            "Total expenditure                   818\n",
            "Diphtheria                           81\n",
            " HIV/AIDS                           200\n",
            "GDP                                2490\n",
            "Population                         2278\n",
            " thinness  1-19 years               200\n",
            " thinness 5-9 years                 207\n",
            "Income composition of resources     625\n",
            "Schooling                           173\n",
            "dtype: int64\n",
            "\n",
            " les colonnes avec 1 seul valeur unique sont :  Index([], dtype='object')\n",
            "\n",
            " les colonnes avec 1 seul valeur unique ont ete supprimee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#identifier et supprimer les colonnes qui ont tres peu de valeurs (variance)\n",
        "seuil_variance = 0.01\n",
        "\n",
        "\n",
        "#calcul et affichage de la variance pour chaque colonne (numerical columns)\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"the data types of each column is : \\n\",df.dtypes)\n",
        "df_numerical = df.select_dtypes(include=['number'])\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"the variance of each column is : \\n\",df_numerical.var())\n",
        "\n",
        "#supprimer les colones avec des variance inferieur que le seuil\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "columns_to_drop = df_numerical.columns[df_numerical.var() < seuil_variance]\n",
        "print(\"les colonnes avec une variance < \",seuil_variance,\" sont : \",columns_to_drop )\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "print(\"les colonnes avec une variance < \",seuil_variance,\" ont ete supprimee\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELKpHHUD4RB4",
        "outputId": "c9b9740e-9167-41c8-d0ef-e063c55940ef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "the data types of each column is : \n",
            " Country                             object\n",
            "Year                                 int64\n",
            "Status                              object\n",
            "Life expectancy                    float64\n",
            "Adult Mortality                    float64\n",
            "infant deaths                        int64\n",
            "Alcohol                            float64\n",
            "percentage expenditure             float64\n",
            "Hepatitis B                        float64\n",
            "Measles                              int64\n",
            " BMI                               float64\n",
            "under-five deaths                    int64\n",
            "Polio                              float64\n",
            "Total expenditure                  float64\n",
            "Diphtheria                         float64\n",
            " HIV/AIDS                          float64\n",
            "GDP                                float64\n",
            "Population                         float64\n",
            " thinness  1-19 years              float64\n",
            " thinness 5-9 years                float64\n",
            "Income composition of resources    float64\n",
            "Schooling                          float64\n",
            "dtype: object\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "the variance of each column is : \n",
            " Year                               2.128753e+01\n",
            "Life expectancy                    9.070405e+01\n",
            "Adult Mortality                    1.544852e+04\n",
            "infant deaths                      1.390666e+04\n",
            "Alcohol                            1.642205e+01\n",
            "percentage expenditure             3.951805e+06\n",
            "Hepatitis B                        6.285057e+02\n",
            "Measles                            1.314983e+08\n",
            " BMI                               4.017633e+02\n",
            "under-five deaths                  2.574277e+04\n",
            "Polio                              5.488733e+02\n",
            "Total expenditure                  6.241601e+00\n",
            "Diphtheria                         5.624919e+02\n",
            " HIV/AIDS                          2.578390e+01\n",
            "GDP                                2.036377e+08\n",
            "Population                         3.722476e+15\n",
            " thinness  1-19 years              1.953812e+01\n",
            " thinness 5-9 years                2.033002e+01\n",
            "Income composition of resources    4.448031e-02\n",
            "Schooling                          1.128234e+01\n",
            "dtype: float64\n",
            "------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------\n",
            "les colonnes avec une variance <  0.01  sont :  Index([], dtype='object')\n",
            "les colonnes avec une variance <  0.01  ont ete supprimee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#identifier et supprimer les lignes qui sont des doublons\n",
        "\n",
        "\n",
        "#afficher les doublons\n",
        "print(\"les doublons sont : \\n\",df[df.duplicated()])\n",
        "\n",
        "#supprimer les doublons\n",
        "df = df.drop_duplicates()\n",
        "print(\"les doublons ont ete supprimee\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPzSpyP54pJo",
        "outputId": "4536689f-1f44-42f1-8f1d-3266863dde88"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "les doublons sont : \n",
            " Empty DataFrame\n",
            "Columns: [Country, Year, Status, Life expectancy , Adult Mortality, infant deaths, Alcohol, percentage expenditure, Hepatitis B, Measles ,  BMI , under-five deaths , Polio, Total expenditure, Diphtheria ,  HIV/AIDS, GDP, Population,  thinness  1-19 years,  thinness 5-9 years, Income composition of resources, Schooling]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 22 columns]\n",
            "les doublons ont ete supprimee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#La gestion des outliers avec IQR ( optionel et depend du model a utiliser apres pour l'entrainement)\n",
        "\n",
        "'''\n",
        "df_numerical = df.select_dtypes(include=['number'])\n",
        "#calculate interquartile range\n",
        "Q1 = df_numerical.quantile(0.25)\n",
        "Q3 = df_numerical.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"the IQR of each column is : \\n\",IQR)\n",
        "\n",
        "\n",
        "#calculate the outlier cutoff\n",
        "cut_off = IQR * 1.5\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"the outlier cutoff is : \\n\",cut_off)\n",
        "lower, upper = Q1 - cut_off, Q3 + cut_off\n",
        "\n",
        "\n",
        "#identify the outliers\n",
        "outliers = [x for x in df_numerical if x < lower[x] or x > upper[x]]\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"the outliers are : \\n\",outliers)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "IwjfC5xcFwsE",
        "outputId": "005804c8-c8c9-4091-de09-9ac069eac4ae"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndf_numerical = df.select_dtypes(include=[\\'number\\'])\\n#calculate interquartile range\\nQ1 = df_numerical.quantile(0.25)\\nQ3 = df_numerical.quantile(0.75)\\nIQR = Q3 - Q1\\nprint(\"------------------------------------------------------------------------------------------\")\\nprint(\"------------------------------------------------------------------------------------------\")\\nprint(\"the IQR of each column is : \\n\",IQR)\\n\\n\\n#calculate the outlier cutoff\\ncut_off = IQR * 1.5\\nprint(\"------------------------------------------------------------------------------------------\")\\nprint(\"------------------------------------------------------------------------------------------\")\\nprint(\"the outlier cutoff is : \\n\",cut_off)\\nlower, upper = Q1 - cut_off, Q3 + cut_off\\n\\n\\n#identify the outliers\\noutliers = [x for x in df_numerical if x < lower[x] or x > upper[x]]\\nprint(\"------------------------------------------------------------------------------------------\")\\nprint(\"------------------------------------------------------------------------------------------\")\\nprint(\"the outliers are : \\n\",outliers)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# la gestion des valeurs manquantes\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "df_numerical = df.select_dtypes(include=[np.number])\n",
        "df_categorical = df.select_dtypes(exclude=[np.number])\n",
        "\n",
        "# Loop through columns\n",
        "for col in df.columns:\n",
        "    missing_percent = df[col].isnull().mean() * 100\n",
        "\n",
        "    if missing_percent < 5:\n",
        "        if col in df_numerical.columns:\n",
        "            imputer = SimpleImputer(strategy='median')\n",
        "            df[[col]] = imputer.fit_transform(df[[col]])\n",
        "        else:\n",
        "            imputer = SimpleImputer(strategy='most_frequent')\n",
        "            df[[col]] = imputer.fit_transform(df[[col]])\n",
        "\n",
        "    elif 5 <= missing_percent <= 30:\n",
        "        if col in df_numerical.columns:\n",
        "            # Using KNN imputer for numerical values\n",
        "            knn_imputer = KNNImputer(n_neighbors=5)\n",
        "            df[[col]] = knn_imputer.fit_transform(df[[col]])\n",
        "        else:\n",
        "            # Use most_frequent as a placeholder for categorical regression\n",
        "            # or consider more advanced imputation techniques (like using RandomForest or logistic regression)\n",
        "            imputer = SimpleImputer(strategy='most_frequent')\n",
        "            df[[col]] = imputer.fit_transform(df[[col]])\n",
        "\n",
        "    else:\n",
        "        df.drop(columns=col, inplace=True)\n",
        "\n",
        "print(\"missing values have been dealt with\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIEJmneMJCoS",
        "outputId": "5616c3d7-4fa6-4197-ead7-b719f4d83836"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missing values have been dealt with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding\n",
        "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "df_numerical = df.select_dtypes(include=[np.number])\n",
        "df_categorical = df.select_dtypes(exclude=[np.number])\n",
        "\n",
        "\n",
        "#for categorical features if unique values <5 use one hot encoding else use label encoder\n",
        "for col in df_categorical.columns:\n",
        "  if df[col].nunique() < 5:\n",
        "    #one hot encoding\n",
        "    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [col])], remainder='passthrough')\n",
        "    df = pd.DataFrame(ct.fit_transform(df))\n",
        "  else:\n",
        "    #label encoding\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n"
      ],
      "metadata": {
        "id": "a1eNC8dGRI7d"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "ss=StandardScaler()\n",
        "df=pd.DataFrame(ss.fit_transform(df))\n",
        "\n"
      ],
      "metadata": {
        "id": "qHtNZEPpSwiK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the cleaned dataframe\n",
        "\n",
        "df.to_csv('cleaned_data.csv', index=False)\n",
        "print(\"the cleaned data has been saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d5-vV4vS7Cf",
        "outputId": "31ef42c6-88d7-4e98-bf64-cbc5b89b301c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the cleaned data has been saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**enhanced preprocessing with logs through out preprocessing**"
      ],
      "metadata": {
        "id": "dNonDp5rUoc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Enhanced Data Preprocessing Template with Comprehensive Reporting\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "class PreprocessingReporter:\n",
        "    def __init__(self, dataset_name):\n",
        "        self.report_lines = []\n",
        "        self.dataset_name = dataset_name\n",
        "        self.start_time = datetime.now()\n",
        "\n",
        "        # Initialize report\n",
        "        self.add_header()\n",
        "\n",
        "    def add_header(self):\n",
        "        \"\"\"Add report header with timestamp and dataset info\"\"\"\n",
        "        self.report_lines.extend([\n",
        "            \"=\"*80,\n",
        "            \"DATA PREPROCESSING REPORT\",\n",
        "            \"=\"*80,\n",
        "            f\"Dataset: {self.dataset_name}\",\n",
        "            f\"Processing Date: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            f\"Generated by: Enhanced Preprocessing Template\",\n",
        "            \"=\"*80,\n",
        "            \"\"\n",
        "        ])\n",
        "\n",
        "    def add_section(self, title):\n",
        "        \"\"\"Add a new section to the report\"\"\"\n",
        "        self.report_lines.extend([\n",
        "            f\"\\n{'-'*60}\",\n",
        "            f\"{title.upper()}\",\n",
        "            f\"{'-'*60}\\n\"\n",
        "        ])\n",
        "\n",
        "    def add_content(self, content):\n",
        "        \"\"\"Add content to the report\"\"\"\n",
        "        if isinstance(content, list):\n",
        "            self.report_lines.extend(content)\n",
        "        else:\n",
        "            self.report_lines.append(str(content))\n",
        "\n",
        "    def add_dataframe_summary(self, df, title):\n",
        "        \"\"\"Add dataframe summary statistics\"\"\"\n",
        "        self.add_content([\n",
        "            f\"\\n{title}:\",\n",
        "            f\"Shape: {df.shape}\",\n",
        "            f\"Columns: {list(df.columns)}\",\n",
        "            f\"Data Types:\\n{df.dtypes.to_string()}\",\n",
        "            f\"\\nMissing Values Summary:\",\n",
        "            f\"{df.isnull().sum().to_string()}\",\n",
        "            f\"\\nDescriptive Statistics:\",\n",
        "            f\"{df.describe().to_string()}\",\n",
        "            \"\"\n",
        "        ])\n",
        "\n",
        "    def save_report(self, filename=\"preprocessing_report.txt\"):\n",
        "        \"\"\"Save the report to a text file\"\"\"\n",
        "        # Add completion timestamp\n",
        "        end_time = datetime.now()\n",
        "        processing_time = end_time - self.start_time\n",
        "\n",
        "        self.report_lines.extend([\n",
        "            \"\\n\" + \"=\"*80,\n",
        "            \"PREPROCESSING COMPLETED\",\n",
        "            \"=\"*80,\n",
        "            f\"End Time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            f\"Total Processing Time: {processing_time}\",\n",
        "            f\"Report saved as: {filename}\",\n",
        "            \"=\"*80\n",
        "        ])\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(self.report_lines))\n",
        "\n",
        "        print(f\"Preprocessing report saved as: {filename}\")\n",
        "\n",
        "def enhanced_preprocessing_pipeline(dataset_path, output_filename=\"cleaned_data.csv\", report_filename=\"preprocessing_report.txt\"):\n",
        "    \"\"\"\n",
        "    Enhanced preprocessing pipeline with comprehensive reporting\n",
        "    \"\"\"\n",
        "    # Initialize reporter\n",
        "    dataset_name = os.path.basename(dataset_path)\n",
        "    reporter = PreprocessingReporter(dataset_name)\n",
        "\n",
        "    # Step 1: Data Import\n",
        "    reporter.add_section(\"1. DATA IMPORT\")\n",
        "    try:\n",
        "        df_original = pd.read_csv(dataset_path)\n",
        "        reporter.add_content([\n",
        "            f\"✓ Dataset imported successfully from: {dataset_path}\",\n",
        "            f\"Original dataset shape: {df_original.shape}\",\n",
        "            \"\"\n",
        "        ])\n",
        "\n",
        "        # Add initial data summary\n",
        "        reporter.add_dataframe_summary(df_original, \"ORIGINAL DATASET SUMMARY\")\n",
        "\n",
        "    except Exception as e:\n",
        "        reporter.add_content(f\"✗ Error importing dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Create a copy for processing\n",
        "    df = df_original.copy()\n",
        "\n",
        "    # Step 2: Remove Single-Value Columns\n",
        "    reporter.add_section(\"2. SINGLE-VALUE COLUMN REMOVAL\")\n",
        "\n",
        "    unique_counts = df.nunique()\n",
        "    columns_to_drop_single = df.columns[unique_counts == 1].tolist()\n",
        "\n",
        "    if columns_to_drop_single:\n",
        "        df = df.drop(columns=columns_to_drop_single)\n",
        "        reporter.add_content([\n",
        "            f\"Decision: Remove columns with only 1 unique value\",\n",
        "            f\"Reason: These columns provide no information for analysis\",\n",
        "            f\"Columns removed: {columns_to_drop_single}\",\n",
        "            f\"Columns dropped: {len(columns_to_drop_single)}\",\n",
        "            f\"Remaining columns: {df.shape[1]}\",\n",
        "            \"\"\n",
        "        ])\n",
        "    else:\n",
        "        reporter.add_content([\n",
        "            \"✓ No single-value columns found\",\n",
        "            \"Decision: No action needed\",\n",
        "            \"\"\n",
        "        ])\n",
        "\n",
        "    # Step 3: Low Variance Column Removal\n",
        "    reporter.add_section(\"3. LOW VARIANCE COLUMN REMOVAL\")\n",
        "\n",
        "    variance_threshold = 0.01\n",
        "    df_numerical = df.select_dtypes(include=['number'])\n",
        "\n",
        "    if not df_numerical.empty:\n",
        "        variances = df_numerical.var()\n",
        "        low_variance_cols = variances[variances < variance_threshold].index.tolist()\n",
        "\n",
        "        reporter.add_content([\n",
        "            f\"Variance threshold: {variance_threshold}\",\n",
        "            f\"Numerical columns analyzed: {len(df_numerical.columns)}\",\n",
        "            f\"Variance summary:\\n{variances.to_string()}\",\n",
        "            \"\"\n",
        "        ])\n",
        "\n",
        "        if low_variance_cols:\n",
        "            df = df.drop(columns=low_variance_cols)\n",
        "            reporter.add_content([\n",
        "                f\"Decision: Remove low variance columns\",\n",
        "                f\"Reason: Low variance columns may not contribute significantly to analysis\",\n",
        "                f\"Columns removed: {low_variance_cols}\",\n",
        "                f\"Columns dropped: {len(low_variance_cols)}\",\n",
        "                f\"Remaining columns: {df.shape[1]}\",\n",
        "                \"\"\n",
        "            ])\n",
        "        else:\n",
        "            reporter.add_content([\n",
        "                \"✓ No low variance columns found\",\n",
        "                \"Decision: No action needed\",\n",
        "                \"\"\n",
        "            ])\n",
        "    else:\n",
        "        reporter.add_content([\n",
        "            \"No numerical columns found for variance analysis\",\n",
        "            \"Decision: Skip variance-based removal\",\n",
        "            \"\"\n",
        "        ])\n",
        "\n",
        "    # Step 4: Duplicate Removal\n",
        "    reporter.add_section(\"4. DUPLICATE ROW REMOVAL\")\n",
        "\n",
        "    duplicate_count = df.duplicated().sum()\n",
        "\n",
        "    if duplicate_count > 0:\n",
        "        df = df.drop_duplicates()\n",
        "        reporter.add_content([\n",
        "            f\"Decision: Remove duplicate rows\",\n",
        "            f\"Reason: Duplicate rows can bias analysis and model training\",\n",
        "            f\"Duplicate rows found: {duplicate_count}\",\n",
        "            f\"Rows after removal: {df.shape[0]}\",\n",
        "            \"\"\n",
        "        ])\n",
        "    else:\n",
        "        reporter.add_content([\n",
        "            \"✓ No duplicate rows found\",\n",
        "            \"Decision: No action needed\",\n",
        "            \"\"\n",
        "        ])\n",
        "\n",
        "    # Step 5: Missing Value Handling\n",
        "    reporter.add_section(\"5. MISSING VALUE HANDLING\")\n",
        "\n",
        "    missing_summary = df.isnull().sum()\n",
        "    missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "    reporter.add_content([\n",
        "        \"Missing value strategy:\",\n",
        "        \"• <5% missing: Simple imputation (median for numerical, mode for categorical)\",\n",
        "        \"• 5-30% missing: Advanced imputation (KNN for numerical, mode for categorical)\",\n",
        "        \"• >30% missing: Drop column\",\n",
        "        \"\",\n",
        "        \"Missing value analysis:\"\n",
        "    ])\n",
        "\n",
        "    for col in df.columns:\n",
        "        missing_count = missing_summary[col]\n",
        "        missing_percent = missing_percentages[col]\n",
        "\n",
        "        if missing_count > 0:\n",
        "            reporter.add_content(f\"  {col}: {missing_count} missing ({missing_percent:.2f}%)\")\n",
        "\n",
        "    reporter.add_content(\"\")\n",
        "\n",
        "    # Separate numerical and categorical columns\n",
        "    df_numerical = df.select_dtypes(include=[np.number])\n",
        "    df_categorical = df.select_dtypes(exclude=[np.number])\n",
        "\n",
        "    columns_dropped_missing = []\n",
        "    columns_imputed = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        missing_percent = (df[col].isnull().sum() / len(df)) * 100\n",
        "\n",
        "        if missing_percent > 30:\n",
        "            # Drop column\n",
        "            df.drop(columns=col, inplace=True)\n",
        "            columns_dropped_missing.append(col)\n",
        "            reporter.add_content(f\"  {col}: DROPPED (>{30}% missing)\")\n",
        "\n",
        "        elif missing_percent > 0:\n",
        "            # Impute\n",
        "            if col in df_numerical.columns:\n",
        "                if missing_percent < 5:\n",
        "                    imputer = SimpleImputer(strategy='median')\n",
        "                    method = \"median imputation\"\n",
        "                else:\n",
        "                    imputer = KNNImputer(n_neighbors=5)\n",
        "                    method = \"KNN imputation\"\n",
        "            else:\n",
        "                imputer = SimpleImputer(strategy='most_frequent')\n",
        "                method = \"mode imputation\"\n",
        "\n",
        "            df[[col]] = imputer.fit_transform(df[[col]])\n",
        "            columns_imputed.append((col, method))\n",
        "            reporter.add_content(f\"  {col}: IMPUTED using {method}\")\n",
        "\n",
        "    reporter.add_content([\n",
        "        \"\",\n",
        "        f\"Summary:\",\n",
        "        f\"  Columns dropped due to missing values: {len(columns_dropped_missing)}\",\n",
        "        f\"  Columns imputed: {len(columns_imputed)}\",\n",
        "        f\"  Remaining columns: {df.shape[1]}\",\n",
        "        \"\"\n",
        "    ])\n",
        "\n",
        "    # Step 6: Categorical Encoding\n",
        "    reporter.add_section(\"6. CATEGORICAL ENCODING\")\n",
        "\n",
        "    df_categorical = df.select_dtypes(exclude=[np.number])\n",
        "\n",
        "    if not df_categorical.empty:\n",
        "        reporter.add_content([\n",
        "            \"Encoding strategy:\",\n",
        "            \"• <5 unique values: One-Hot Encoding\",\n",
        "            \"• ≥5 unique values: Label Encoding\",\n",
        "            \"\",\n",
        "            \"Categorical columns analysis:\"\n",
        "        ])\n",
        "\n",
        "        encoding_actions = []\n",
        "\n",
        "        for col in df_categorical.columns:\n",
        "            unique_count = df[col].nunique()\n",
        "\n",
        "            if unique_count < 5:\n",
        "                # One-hot encoding\n",
        "                ct = ColumnTransformer(\n",
        "                    transformers=[('encoder', OneHotEncoder(drop='first', sparse_output=False), [col])],\n",
        "                    remainder='passthrough'\n",
        "                )\n",
        "                df_encoded = ct.fit_transform(df)\n",
        "\n",
        "                # Get feature names\n",
        "                feature_names = ct.named_transformers_['encoder'].get_feature_names_out([col]).tolist()\n",
        "                feature_names.extend([c for c in df.columns if c != col])\n",
        "\n",
        "                df = pd.DataFrame(df_encoded, columns=feature_names)\n",
        "                encoding_actions.append(f\"  {col}: ONE-HOT ENCODED ({unique_count} unique values)\")\n",
        "\n",
        "            else:\n",
        "                # Label encoding\n",
        "                le = LabelEncoder()\n",
        "                df[col] = le.fit_transform(df[col])\n",
        "                encoding_actions.append(f\"  {col}: LABEL ENCODED ({unique_count} unique values)\")\n",
        "\n",
        "        reporter.add_content(encoding_actions)\n",
        "        reporter.add_content([\n",
        "            \"\",\n",
        "            f\"Final shape after encoding: {df.shape}\",\n",
        "            \"\"\n",
        "        ])\n",
        "\n",
        "    else:\n",
        "        reporter.add_content([\n",
        "            \"✓ No categorical columns found\",\n",
        "            \"Decision: No encoding needed\",\n",
        "            \"\"\n",
        "        ])\n",
        "\n",
        "    # Step 7: Feature Scaling\n",
        "    reporter.add_section(\"7. FEATURE SCALING\")\n",
        "\n",
        "    reporter.add_content([\n",
        "        \"Decision: Apply StandardScaler to all features\",\n",
        "        \"Reason: Ensures all features have mean=0 and std=1 for better model performance\",\n",
        "        \"\"\n",
        "    ])\n",
        "\n",
        "    # Get column names before scaling\n",
        "    column_names = df.columns.tolist()\n",
        "\n",
        "    # Apply scaling\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(df)\n",
        "    df = pd.DataFrame(df_scaled, columns=column_names)\n",
        "\n",
        "    reporter.add_content([\n",
        "        f\"✓ StandardScaler applied to all {df.shape[1]} features\",\n",
        "        f\"Final dataset shape: {df.shape}\",\n",
        "        \"\"\n",
        "    ])\n",
        "\n",
        "    # Step 8: Final Summary\n",
        "    reporter.add_section(\"8. FINAL DATASET SUMMARY\")\n",
        "    reporter.add_dataframe_summary(df, \"CLEANED DATASET SUMMARY\")\n",
        "\n",
        "    # Step 9: Save Results\n",
        "    reporter.add_section(\"9. SAVING RESULTS\")\n",
        "\n",
        "    try:\n",
        "        df.to_csv(output_filename, index=False)\n",
        "        reporter.add_content([\n",
        "            f\"✓ Cleaned dataset saved as: {output_filename}\",\n",
        "            f\"Final shape: {df.shape}\",\n",
        "            \"\"\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        reporter.add_content(f\"✗ Error saving cleaned dataset: {e}\")\n",
        "\n",
        "    # Save report\n",
        "    reporter.save_report(report_filename)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    dataset_path = 'Life Expectancy Data.csv'\n",
        "    output_file = 'cleaned_data.csv'\n",
        "    report_file = 'preprocessing_report.txt'\n",
        "\n",
        "    # Run preprocessing pipeline\n",
        "    print(\"Starting enhanced preprocessing pipeline...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    cleaned_data = enhanced_preprocessing_pipeline(\n",
        "        dataset_path=dataset_path,\n",
        "        output_filename=output_file,\n",
        "        report_filename=report_file\n",
        "    )\n",
        "\n",
        "    if cleaned_data is not None:\n",
        "        print(f\"\\n✓ Preprocessing completed successfully!\")\n",
        "        print(f\"✓ Cleaned data saved as: {output_file}\")\n",
        "        print(f\"✓ Detailed report saved as: {report_file}\")\n",
        "        print(f\"✓ Final dataset shape: {cleaned_data.shape}\")\n",
        "    else:\n",
        "        print(\"\\n✗ Preprocessing failed. Check the report for details.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_-H5J5eUqqt",
        "outputId": "54e4529e-6fbb-4e07-b8a4-a19be95f4dc2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting enhanced preprocessing pipeline...\n",
            "==================================================\n",
            "Preprocessing report saved as: preprocessing_report.txt\n",
            "\n",
            "✓ Preprocessing completed successfully!\n",
            "✓ Cleaned data saved as: cleaned_data.csv\n",
            "✓ Detailed report saved as: preprocessing_report.txt\n",
            "✓ Final dataset shape: (2938, 22)\n"
          ]
        }
      ]
    }
  ]
}